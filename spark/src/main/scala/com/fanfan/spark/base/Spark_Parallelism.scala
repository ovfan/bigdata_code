package com.fanfan.spark.base

import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object Spark_Parallelism {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("Test").setMaster("local[*]")
    // é…ç½®å¯¹è±¡ä¸­è®¾ç½®åˆ†åŒºæ•°é‡
    conf.set("spark.default.parallelism", "6")

    val sc = new SparkContext(conf)
    // RDDåˆ†åŒºæ•°é‡çš„åˆ¤æ–­ä¾æ®

    // 1. æ‰‹åŠ¨è®¾ç½®åˆ†åŒºæ•°é‡

    val rdd: RDD[Int] = sc.makeRDD(Seq(1, 2, 3, 4, 5, 6), 3) // è®¾ç½®3ä¸ªåˆ†åŒºï¼Œåˆ†åŒºæ‰èƒ½æé«˜è®¡ç®—çš„å¹¶è¡Œåº¦
    // ä¼˜å…ˆçº§ æ˜¯ æ–¹æ³•ä¸­è®¾ç½®åˆ†åŒº > é…ç½®å¯¹è±¡

    // éªŒè¯ï¼šå°†æ•°æ®åˆ†åŒºä¿å­˜æˆæ–‡æœ¬æ–‡ä»¶
    rdd.saveAsTextFile("spark/src/main/resources/r1")

    //æœ€ç»ˆåˆ†åŒºæ•°é‡ä¸º: ğŸ‘‰ 3ä¸ª
    sc.stop()
  }
}
